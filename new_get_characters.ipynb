{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "api_key = config['OPENAI_API_KEY']\n",
    "chroma_path = config['CHROMA_PATH']\n",
    "chroma_collection = config['CHROMA_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SERIES_ID = 3 # wheel of time\n",
    "TARGET_BOOK_NUMBER = 4\n",
    "\n",
    "\n",
    "# Load series.yml to create a mapping from series_metadata_name to series_id\n",
    "with open('series.yml', 'r') as file:\n",
    "    series_list = yaml.safe_load(file)\n",
    "\n",
    "target_series_info = next(series for series in series_list if series['series_id'] == TARGET_SERIES_ID)\n",
    "target_book_info = next(book for book in target_series_info['books'] if book['number_in_series'] == TARGET_BOOK_NUMBER)\n",
    "\n",
    "series_metadata_name = target_series_info['series_metadata_name']\n",
    "book_metadata_name = target_book_info['book_metadata_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run queries through story_sage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from story_sage.story_sage import StorySage\n",
    "\n",
    "# Configure the logger\n",
    "\n",
    "logger = logging.getLogger('story_sage')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it for the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Filter out logs from other modules\n",
    "class StorySageFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name.startswith('story_sage')\n",
    "\n",
    "logger.addFilter(StorySageFilter())\n",
    "\n",
    "# Load all character dictionaries and merge them using the metadata_to_id mapping\n",
    "# Load entities.json\n",
    "with open('entities.json', 'r') as file:\n",
    "    entities = yaml.safe_load(file)\n",
    "\n",
    "story_sage = StorySage(\n",
    "    api_key=api_key,\n",
    "    chroma_path=chroma_path,\n",
    "    chroma_collection_name=chroma_collection,\n",
    "    entities=entities,\n",
    "    series_yml_path='series.yml',\n",
    "    n_chunks=10\n",
    ")\n",
    "\n",
    "\n",
    "# Add a handler to the StorySage logger\n",
    "story_sage.logger = logger\n",
    "\n",
    "def invoke_story_sage(data: dict):\n",
    "    required_keys = ['question', 'book_number', 'chapter_number', 'series_id']\n",
    "    if not all(key in data for key in required_keys):\n",
    "        return {'error': f'Missing parameter! Request must include {\", \".join(required_keys)}'}, 400\n",
    "\n",
    "    try:\n",
    "        result, context = story_sage.invoke(**data)\n",
    "        return result, context\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return {'error': 'Internal server error.'}, 500\n",
    "    \n",
    "data = {\n",
    "    'question': 'Explain the interactions between Cenn and Rand',\n",
    "    'book_number': 2,\n",
    "    'chapter_number': 1,\n",
    "    'series_id': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response, context = invoke_story_sage(data)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and send queries to ChromaDB Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "class EmbeddingAdapter(SentenceTransformerEmbeddings):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)  \n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self._embed_documents(input)  \n",
    "\n",
    "embedder = EmbeddingAdapter\n",
    "client = chromadb.PersistentClient(path=chroma_path)\n",
    "vector_store = client.get_collection(name=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'$and': [\n",
    "                {'$or': [\n",
    "                    {'book_number': {'$lt': 1}},\n",
    "                    {'$and': [\n",
    "                        {'book_number': 1}, \n",
    "                        {'chapter_number': {'$lt': 25}}\n",
    "                    ]}\n",
    "                ]}, \n",
    "                {'3_e_8': True}\n",
    "               ]}\n",
    "\n",
    "# filter_dict = {'$or': [\n",
    "#                     {'book_number': {'$lt': 1}},\n",
    "#                     {'$and': [\n",
    "#                         {'book_number': 1}, \n",
    "#                         {'chapter_number': {'$lt': 25}}\n",
    "#                     ]}\n",
    "#                 ]}\n",
    "#client.delete_collection('wot_retriever_test')\n",
    "if False:\n",
    "    result = vector_store.query(query_texts=['trolloc'],\n",
    "                                n_results=5,\n",
    "                                where=filter_dict,\n",
    "                                include=['metadatas','documents'])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New entity extraction\n",
    "\n",
    "Load chunks dict:\n",
    "\n",
    "```python\n",
    "chunks = {\n",
    "    int: {\n",
    "        int: List[str]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "where keys are `chunks[book_number][chapter_number]` and values are lists of strings, each string is a chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "path_to_chunks = f'./chunks/{series_metadata_name}/semantic_chunks'\n",
    "chunks = {}\n",
    "for filepath in glob.glob(f'{path_to_chunks}/*.pkl'):\n",
    "    match = re.match(r'(\\d+)_(\\d+)\\.pkl', os.path.basename(filepath))\n",
    "    if match:\n",
    "        book_number, chapter_number = map(int, match.groups())\n",
    "        with open(filepath, 'rb') as f:\n",
    "            if book_number not in chunks:\n",
    "                chunks[book_number] = {}\n",
    "            chunks[book_number][chapter_number] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI based entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "# Create a custom HTTPX client with SSL verification disabled\n",
    "req_client = httpx.Client(verify=False)\n",
    "\n",
    "client = OpenAI(api_key=api_key, http_client=req_client)\n",
    "\n",
    "full_response = None\n",
    "\n",
    "class StorySageEntities(BaseModel):\n",
    "  people: list[str]\n",
    "  places: list[str]\n",
    "  groups: list[str]\n",
    "  animals: list[str]\n",
    "  objects: list[str]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                extract entities and a summary from text provided to you.\n",
    "                \n",
    "                For example, if the text is:\n",
    "                    Standing with the other Whitecloaks, Perrin saw the Lugard Road near the Manetherendrelle and the border of Murandy.\n",
    "                    If dogs had been able to make footprints on stone, he would have said the tracks were the prints of a pack of large hounds.\n",
    "                    He hefted his axe and kicked aside the basket on the road.\n",
    "             \n",
    "                Extract:\n",
    "                    People: Perrin\n",
    "                    Places: Lugard Road, Manetherendrelle, Murandy\n",
    "                    Groups: Whitecloaks, pack\n",
    "                    Animals: dogs\n",
    "                    Objects: axe, basket\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=StorySageEntities\n",
    "    )\n",
    "\n",
    "    extracted_entity = completion.choices[0].message.parsed\n",
    "    usage_information = completion.usage\n",
    "\n",
    "    return extracted_entity, usage_information\n",
    "\n",
    "def extract_entities_from_chunks(book_chunks: dict, token_per_min_limit: int = 200000, cooldown_secs: int = 30) -> list:\n",
    "    \"\"\"\n",
    "    Extract named entities from chunks of text in a book.\n",
    "    Args:\n",
    "        book_chunks (dict): A dictionary where keys are chapter indices and values are lists of text chunks.\n",
    "        token_per_min_limit (int, optional): The limit on the number of tokens processed per minute. Defaults to 200000.\n",
    "        cooldown_secs (int, optional): The cooldown period in seconds to wait if the character limit is exceeded. Defaults to 30.\n",
    "    Raises:\n",
    "        ValueError: If cooldown_secs is greater than 30.\n",
    "    Returns:\n",
    "        list: A list of extracted named entities from each chapter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Raise an error if cooldown_secs > 30\n",
    "    if cooldown_secs > 30:\n",
    "        raise ValueError('Cooldown seconds cannot exceed 30 seconds.')\n",
    "    \n",
    "    # Calculate the number of chapters and initialize an empty list to store results\n",
    "    num_chapters = len(book_chunks)\n",
    "    result = []\n",
    "\n",
    "    # Set a limit on the number of tokens processed per minute\n",
    "    len_cap = (token_per_min_limit * 4) / (60 / cooldown_secs)  # ~ 4 characters per token, divide by 2 for 30s cooldown\n",
    "    \n",
    "    # Keep track of the number of text characters processed\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over the chapters\n",
    "    for i, chapter_chunks in book_chunks.items():\n",
    "        # Extract the chapter chunks and join them into a single text\n",
    "        chapter_text = '\\n'.join(chapter_chunks)\n",
    "\n",
    "        # Check if the chapter length exceeds the limit and wait if necessary\n",
    "        chapter_len = len(chapter_text)\n",
    "        if counter + chapter_len > len_cap:\n",
    "            print(f'Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: {i + 1}. Current length: {counter}')\n",
    "            time.sleep(30)\n",
    "            counter = 0\n",
    "        \n",
    "        # Extract named entities from the chapter text\n",
    "        result.append(extract_named_entities(chapter_text))\n",
    "\n",
    "        # Update the character counter\n",
    "        counter += chapter_len\n",
    "\n",
    "    print(f'Finished extracting from {num_chapters} chapters')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump result into a json so I don't have to run this every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "series_info = {\n",
    "    'series_id': 3,\n",
    "    'series_name': 'Wheel of Time',\n",
    "    'series_metadata_name': 'wheel_of_time',\n",
    "    'books': [{\n",
    "        'number_in_series': 1,\n",
    "        'title': 'The Eye of the World',\n",
    "        'book_metadata_name': '01_the_eye_of_the_world',\n",
    "        'number_of_chapters': 53\n",
    "    }]\n",
    "}\n",
    "\n",
    "target_book_info = {\n",
    "    'number_in_series': 2,\n",
    "    'title': 'The Great Hunt',\n",
    "    'book_metadata_name': '02_the_great_hunt',\n",
    "    'number_of_chapters': 50\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: 46. Current length: 396248\n",
      "Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: 21. Current length: 373705\n",
      "Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: 25. Current length: 372095\n",
      "Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: 40. Current length: 377324\n",
      "Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: 2. Current length: 341166\n",
      "Finished extracting from 58 chapters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "target_file_path = f'./entities/{series_metadata_name}'\n",
    "if not os.path.exists(target_file_path):\n",
    "    os.makedirs(target_file_path)\n",
    "target_filename = f'{target_file_path}/{book_metadata_name}.json'\n",
    "\n",
    "# Extract entities from the chunks of the target book\n",
    "if True:\n",
    "    extracted_entities_dict = extract_entities_from_chunks(chunks[TARGET_BOOK_NUMBER])\n",
    "    with open(target_filename, 'w') as json_file:\n",
    "        json.dump(extracted_entities_dict, json_file, default=lambda o: o.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "extracted_entities_dict = [\n",
    "    [\n",
    "        {\n",
    "            'people': List[str],\n",
    "            'places': List[str],\n",
    "            'groups': List[str],\n",
    "            'animals': List[str],\n",
    "            'objects': List[str],\n",
    "        },\n",
    "        {\n",
    "            <OpenAI Usage information>\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "where index in `extracted_entities_dict` corresponds to the chapter number (chapter 0 is anything before chapter 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all extracted entities from the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unique_values(extracted_entities_dict: dict) -> tuple[list, list]:\n",
    "    series_people_set = set()\n",
    "    series_entities_set = set()\n",
    "    \n",
    "    for chapter in extracted_entities_dict:\n",
    "        entities = chapter[0]\n",
    "        series_people_set.update(entities.get('people', []))\n",
    "        \n",
    "        for key, values in entities.items():\n",
    "            if key != 'people':\n",
    "                series_entities_set.update(values)\n",
    "\n",
    "    series_people_list = []\n",
    "    series_entities_list = []\n",
    "    \n",
    "    for person in series_people_set:\n",
    "        person = person.lower()\n",
    "        person = ''.join(c for c in person if c.isalpha() or c.isspace())\n",
    "        series_people_list.append(person)\n",
    "\n",
    "    for entity in series_entities_set:\n",
    "        entity = entity.lower()\n",
    "        entity = ''.join(c for c in entity if c.isalpha() or c.isspace())\n",
    "        series_entities_list.append(entity)\n",
    "    \n",
    "    return series_people_list, series_entities_list\n",
    "\n",
    "series_people_list, series_entities_list = collect_unique_values(extracted_entities_dict=extracted_entities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to cluster similar characters\n",
    "\n",
    "This is an opportunity to make it MUCH smarter. Think things like using semantic understanding to differentiate characters who appear together as separate individuals or other things like that.\n",
    "\n",
    "Maybe do multiple steps? OpenAI cluster, then bounce those against the text to see if it sounds like they're different characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GroupedEntities(BaseModel):\n",
    "    entities: list[list[str]]\n",
    "\n",
    "def group_similar_names(names_to_group: list[str]) -> GroupedEntities:\n",
    "    text = ', '.join(names_to_group)\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                group together names that represent the same thing from the text\n",
    "                provided to you.\n",
    "             \n",
    "                Make sure all names in the input are present in the output.   \n",
    "             \n",
    "                For example:\n",
    "                    Input: Bran, Mat, Bran al'Vere, Haral Luhhan, Breyan, Matrim Cauthon, Alsbet Luhhan, Master al'Vere, Mat Cauthon\n",
    "                    Output: [['Bran', \"Bran al'Vere\", \"Master al'Vere\"], ['Mat', 'Matrim Cauthon', 'Mat Cauthon'], ['Breyan'], ['Haral Luhhan'], ['Alsbet Luhhan']]\n",
    "             \n",
    "                Another example:\n",
    "                    Input: sword, axe, horse, spear, mare\n",
    "                    Output: [['sword', 'axe', 'spear'], ['horse', 'mare']]\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=GroupedEntities\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_people = group_similar_names(series_people_list)\n",
    "grouped_entities = group_similar_names(series_entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def remove_duplicate_elements(grouped_entities: GroupedEntities) -> List[List[str]]:\n",
    "    # Create a set to track seen names\n",
    "    seen_names = set()\n",
    "    filtered_groups = []\n",
    "\n",
    "    # Iterate through each group in grouped_entities\n",
    "    for group in grouped_entities.entities:\n",
    "        # Filter out any names we've seen before\n",
    "        filtered_group = []\n",
    "        for name in group:\n",
    "            if name not in seen_names:\n",
    "                filtered_group.append(name)\n",
    "                seen_names.add(name)\n",
    "                \n",
    "        # Only keep groups that still have elements after filtering\n",
    "        if filtered_group:\n",
    "            filtered_groups.append(filtered_group)\n",
    "\n",
    "    return filtered_groups\n",
    "\n",
    "deduped_people = remove_duplicate_elements(grouped_people)\n",
    "deduped_entities = remove_duplicate_elements(grouped_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities into Series-level Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_dict(people, entities, base_id):\n",
    "    result = {\n",
    "        'people_by_id': {},\n",
    "        'people_by_name': {},\n",
    "        'entity_by_id': {},\n",
    "        'entity_by_name': {}\n",
    "    }\n",
    "    \n",
    "    # Populate people_by_id and people_by_name\n",
    "    for i, person_list in enumerate(people):\n",
    "        person_id = f\"{base_id}_p_{i}\"\n",
    "        result['people_by_id'][person_id] = person_list\n",
    "        for name in person_list:\n",
    "            result['people_by_name'][name] = person_id\n",
    "    \n",
    "    # Populate entity_by_id and entity_by_name\n",
    "    for j, entity_list in enumerate(entities):\n",
    "        filtered_entities = [entity for entity in entity_list if entity not in result['people_by_name']]\n",
    "        if filtered_entities:\n",
    "            entity_id = f\"{base_id}_e_{j}\"\n",
    "            result['entity_by_id'][entity_id] = filtered_entities\n",
    "            for entity in filtered_entities:\n",
    "                result['entity_by_name'][entity] = entity_id\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_entities = create_result_dict(deduped_people, deduped_entities, TARGET_SERIES_ID)\n",
    "\n",
    "entities_dict['series'][str(series_id)]['series_entities'] = series_entities\n",
    "\n",
    "with open('entities.json', 'w') as json_file:\n",
    "    json.dump(entities_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story_sage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
