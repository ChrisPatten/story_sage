{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "api_key = config['OPENAI_API_KEY']\n",
    "chroma_path = config['CHROMA_PATH']\n",
    "chroma_collection = config['CHROMA_COLLECTION']\n",
    "\n",
    "# Load series.yml to create a mapping from series_metadata_name to series_id\n",
    "with open('series.yml', 'r') as file:\n",
    "    series_list = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run queries through story_sage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from story_sage.story_sage import StorySage\n",
    "\n",
    "# Configure the logger\n",
    "\n",
    "logger = logging.getLogger('story_sage')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it for the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Filter out logs from other modules\n",
    "class StorySageFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name.startswith('story_sage')\n",
    "\n",
    "logger.addFilter(StorySageFilter())\n",
    "\n",
    "# Load all character dictionaries and merge them using the metadata_to_id mapping\n",
    "# Load entities.json\n",
    "with open('entities.json', 'r') as file:\n",
    "    entities = yaml.safe_load(file)\n",
    "\n",
    "story_sage = StorySage(\n",
    "    api_key=api_key,\n",
    "    chroma_path=chroma_path,\n",
    "    chroma_collection_name=chroma_collection,\n",
    "    entities=entities,\n",
    "    series_yml_path='series.yml',\n",
    "    n_chunks=10\n",
    ")\n",
    "\n",
    "\n",
    "# Add a handler to the StorySage logger\n",
    "story_sage.logger = logger\n",
    "\n",
    "def invoke_story_sage(data: dict):\n",
    "    required_keys = ['question', 'book_number', 'chapter_number', 'series_id']\n",
    "    if not all(key in data for key in required_keys):\n",
    "        return {'error': f'Missing parameter! Request must include {\", \".join(required_keys)}'}, 400\n",
    "\n",
    "    try:\n",
    "        result, context = story_sage.invoke(**data)\n",
    "        return result, context\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return {'error': 'Internal server error.'}, 500\n",
    "    \n",
    "data = {\n",
    "    'question': 'Explain the interactions between Cenn and Rand',\n",
    "    'book_number': 2,\n",
    "    'chapter_number': 1,\n",
    "    'series_id': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 12:26:05,667 - story_sage - INFO - Processing question: Explain the interactions between Cenn and Rand\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interactions between Cenn and Rand primarily revolve around Cenn's role in the village and his often grumpy demeanor, which contrasts with Rand's more youthful perspective. Here are some key points from the excerpts that illustrate their interactions:\n",
      "\n",
      "- **Cenn's Skepticism**: Cenn expresses skepticism about the village festivities and the expenses related to them, indicating a serious side that contrasts with the more carefree attitudes of the younger characters like Rand and Mat.\n",
      "  - *“I still say it’s a foolish waste of money. And those fireworks you all insisted on sending off for.”* (Book 1, Chapter 1)\n",
      "\n",
      "- **Cenn's Authority**: Cenn is shown to be a member of the Village Council, and he often asserts his views strongly. This authority can lead to tension, especially when others, including Rand's father, challenge him.\n",
      "  - *“Act your age,” Bran added. “And for once remember you’re a member of the Council.”* (Book 1, Chapter 4)\n",
      "\n",
      "- **Rand's Perspective**: Rand recognizes Cenn as an elder and reflects on their differences in age and experience, suggesting that he views Cenn's grumpy nature as somewhat outdated.\n",
      "  - *“I guess if I live as long as old Cenn Buie, that’ll be long enough for anybody.”* (Book 1, Chapter 36)\n",
      "\n",
      "- **Cenn and Community Concerns**: Cenn often voices concerns about the well-being of the village, which shows his protective nature, though it can come off as overly cautious or pessimistic.\n",
      "  - *“Mock if you will,” Cenn muttered, “but if it doesn’t warm enough for crops to sprout soon, more than one root cellar will come up empty before there’s a harvest.”* (Book 1, Chapter 1)\n",
      "\n",
      "Overall, Cenn serves as a foil to Rand and his friends, representing the older generation's concerns amidst the younger generation's more rebellious and carefree spirit.\n"
     ]
    }
   ],
   "source": [
    "response, context = invoke_story_sage(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and send queries to ChromaDB Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "class EmbeddingAdapter(SentenceTransformerEmbeddings):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)  \n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self._embed_documents(input)  \n",
    "\n",
    "embedder = EmbeddingAdapter\n",
    "client = chromadb.PersistentClient(path=chroma_path)\n",
    "vector_store = client.get_collection(name=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'$and': [\n",
    "                {'$or': [\n",
    "                    {'book_number': {'$lt': 1}},\n",
    "                    {'$and': [\n",
    "                        {'book_number': 1}, \n",
    "                        {'chapter_number': {'$lt': 25}}\n",
    "                    ]}\n",
    "                ]}, \n",
    "                {'3_e_8': True}\n",
    "               ]}\n",
    "\n",
    "# filter_dict = {'$or': [\n",
    "#                     {'book_number': {'$lt': 1}},\n",
    "#                     {'$and': [\n",
    "#                         {'book_number': 1}, \n",
    "#                         {'chapter_number': {'$lt': 25}}\n",
    "#                     ]}\n",
    "#                 ]}\n",
    "#client.delete_collection('wot_retriever_test')\n",
    "if True:\n",
    "    result = vector_store.query(query_texts=['trolloc'],\n",
    "                                n_results=5,\n",
    "                                where=filter_dict,\n",
    "                                include=['metadatas','documents'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "path_to_chunks = './chunks/wheel_of_time/semantic_chunks'\n",
    "chunks = {}\n",
    "for filepath in glob.glob(f'{path_to_chunks}/*.pkl'):\n",
    "    match = re.match(r'(\\d+)_(\\d+)\\.pkl', os.path.basename(filepath))\n",
    "    if match:\n",
    "        book_number, chapter_number = map(int, match.groups())\n",
    "        with open(filepath, 'rb') as f:\n",
    "            if book_number not in chunks:\n",
    "                chunks[book_number] = {}\n",
    "            chunks[book_number][chapter_number] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI based entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Change this to only extract \"people\" and \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "# Create a custom HTTPX client with SSL verification disabled\n",
    "req_client = httpx.Client(verify=False)\n",
    "\n",
    "client = OpenAI(api_key=api_key, http_client=req_client)\n",
    "\n",
    "full_response = None\n",
    "\n",
    "class StorySageEntities(BaseModel):\n",
    "  people: list[str]\n",
    "  places: list[str]\n",
    "  groups: list[str]\n",
    "  animals: list[str]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                extract entities and a summary from text provided to you.\n",
    "                \n",
    "                For example, if the text is:\n",
    "                    Standing with the other Whitecloaks, Perrin saw the Lugard Road near the Manetherendrelle and the border of Murandy.\n",
    "                    If dogs had been able to make footprints on stone, he would have said the tracks were the prints of a pack of large hounds.\n",
    "             \n",
    "                Extract:\n",
    "                    People: Perrin\n",
    "                    Places: Lugard Road, Manetherendrelle, Murandy\n",
    "                    Groups: Whitecloaks, pack\n",
    "                    Animals: dogs\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=StorySageEntities\n",
    "    )\n",
    "\n",
    "    extracted_entity = completion.choices[0].message.parsed\n",
    "    usage_information = completion.usage\n",
    "\n",
    "    return extracted_entity, usage_information\n",
    "\n",
    "\n",
    "#entities = extract_named_entities(chunk_to_extract)\n",
    "#print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run chunks thru OpenAI for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def extract_entities_from_chunks(chunks):\n",
    "\n",
    "    num_chapters = len(chunks[1])\n",
    "    result = []\n",
    "    \n",
    "    counter = 0\n",
    "    len_cap = 400000\n",
    "    book_chunks = chunks[1]\n",
    "    for i in range(num_chapters):\n",
    "        chapter_chunks = book_chunks[i]\n",
    "        chapter_text = '\\n'.join(chapter_chunks)\n",
    "        chapter_len = len(chapter_text)\n",
    "        if counter + chapter_len > len_cap:\n",
    "            print(f'Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: {i + 1}. Current length: {counter}')\n",
    "            time.sleep(30)\n",
    "            counter = 0\n",
    "        result.append(extract_named_entities(chapter_text))\n",
    "        counter += chapter_len\n",
    "\n",
    "    print(f'Finished extracting from {len(result)} chapters')\n",
    "    if result:\n",
    "        print(result[-5])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump result into a json so I don't have to run this every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if False:\n",
    "    result = extract_entities_from_chunks(chunks)\n",
    "    with open('01_the_eye_of_the_world.json', 'w') as json_file:\n",
    "        json.dump(result, json_file, default=lambda o: o.__dict__, indent=4)\n",
    "\n",
    "with open('01_the_eye_of_the_world.json', 'r') as json_file:\n",
    "    result = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chapters = len(result[0])\n",
    "\n",
    "entities_dict = {\n",
    "    'series': {\n",
    "        3: {\n",
    "            'series_metadata_name': 'wheel_of_time', \n",
    "            'series_id': 3, \n",
    "            'series_name': 'The Wheel of Time',\n",
    "            'series_entities': {\n",
    "                'people_by_id': {}, \n",
    "                'people_by_name': {}, \n",
    "                'entities_by_id': {}, \n",
    "                'entities_by_name': {}\n",
    "            },\n",
    "            'books': [\n",
    "                {\n",
    "                    'book_name': 'The Eye of the World', \n",
    "                    'book_number': 1, \n",
    "                    'chapter_count': num_chapters,\n",
    "                    'chapters': []\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def collect_unique_values(result):\n",
    "    series_people_set = set()\n",
    "    series_entities_set = set()\n",
    "    \n",
    "    for chapter in result:\n",
    "        entities = chapter[0]\n",
    "        series_people_set.update(entities.get('people', []))\n",
    "        \n",
    "        for key, values in entities.items():\n",
    "            if key != 'people':\n",
    "                series_entities_set.update(values)\n",
    "\n",
    "    series_people_list = []\n",
    "    series_entities_list = []\n",
    "    \n",
    "    for person in series_people_set:\n",
    "        person = person.lower()\n",
    "        person = ''.join(c for c in person if c.isalpha() or c.isspace())\n",
    "        series_people_list.append(person)\n",
    "\n",
    "    for entity in series_entities_set:\n",
    "        entity = entity.lower()\n",
    "        entity = ''.join(c for c in entity if c.isalpha() or c.isspace())\n",
    "        series_entities_list.append(entity)\n",
    "    \n",
    "    return series_people_list, series_entities_list\n",
    "\n",
    "series_people_list, series_entities_list = collect_unique_values(result)\n",
    "\n",
    "if True:\n",
    "    with open('entities.json', 'w') as json_file:\n",
    "        json.dump(entities_dict, json_file, indent=4)\n",
    "\n",
    "with open('entities.json', 'r') as json_file:\n",
    "    entities_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to cluster similar characters\n",
    "\n",
    "This is an opportunity to make it MUCH smarter. Think things like using semantic understanding to differentiate characters who appear together as separate individuals or other things like that.\n",
    "\n",
    "Maybe do multiple steps? OpenAI cluster, then bounce those against the text to see if it sounds like they're different characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GroupedEntities(BaseModel):\n",
    "    entities: list[list[str]]\n",
    "\n",
    "def group_similar_names(names_to_group):\n",
    "    text = ', '.join(names_to_group)\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                group together names that represent the same thing from the text\n",
    "                provided to you.\n",
    "             \n",
    "                Make sure all names in the input are present in the output.   \n",
    "             \n",
    "                For example:\n",
    "                    Input: Bran, Mat, Bran al'Vere, Haral Luhhan, Breyan, Matrim Cauthon, Alsbet Luhhan, Master al'Vere, Mat Cauthon\n",
    "                    Output: [['Bran', \"Bran al'Vere\", \"Master al'Vere\"], ['Mat', 'Matrim Cauthon', 'Mat Cauthon'], ['Breyan'], ['Haral Luhhan'], ['Alsbet Luhhan']]\n",
    "             \n",
    "                Another example:\n",
    "                    Input: sword, axe, horse, spear, mare\n",
    "                    Output: [['sword', 'axe', 'spear'], ['horse', 'mare']]\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=GroupedEntities\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_people = group_similar_names(series_people_list)\n",
    "grouped_entities = group_similar_names(series_entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def remove_duplicate_elements(grouped_entities: GroupedEntities) -> List[List[str]]:\n",
    "    # Create a set to track seen names\n",
    "    seen_names = set()\n",
    "    filtered_groups = []\n",
    "\n",
    "    # Iterate through each group in grouped_entities\n",
    "    for group in grouped_entities.entities:\n",
    "        # Filter out any names we've seen before\n",
    "        filtered_group = []\n",
    "        for name in group:\n",
    "            if name not in seen_names:\n",
    "                filtered_group.append(name)\n",
    "                seen_names.add(name)\n",
    "                \n",
    "        # Only keep groups that still have elements after filtering\n",
    "        if filtered_group:\n",
    "            filtered_groups.append(filtered_group)\n",
    "\n",
    "    return filtered_groups\n",
    "\n",
    "deduped_people = remove_duplicate_elements(grouped_people)\n",
    "deduped_entities = remove_duplicate_elements(grouped_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities into Series-level Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_dict(people, entities, base_id):\n",
    "    result = {\n",
    "        'people_by_id': {},\n",
    "        'people_by_name': {},\n",
    "        'entity_by_id': {},\n",
    "        'entity_by_name': {}\n",
    "    }\n",
    "    \n",
    "    # Populate people_by_id and people_by_name\n",
    "    for i, person_list in enumerate(people):\n",
    "        person_id = f\"{base_id}_p_{i}\"\n",
    "        result['people_by_id'][person_id] = person_list\n",
    "        for name in person_list:\n",
    "            result['people_by_name'][name] = person_id\n",
    "    \n",
    "    # Populate entity_by_id and entity_by_name\n",
    "    for j, entity_list in enumerate(entities):\n",
    "        filtered_entities = [entity for entity in entity_list if entity not in result['people_by_name']]\n",
    "        if filtered_entities:\n",
    "            entity_id = f\"{base_id}_e_{j}\"\n",
    "            result['entity_by_id'][entity_id] = filtered_entities\n",
    "            for entity in filtered_entities:\n",
    "                result['entity_by_name'][entity] = entity_id\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_id = 3\n",
    "series_entities = create_result_dict(deduped_people, deduped_entities, series_id)\n",
    "\n",
    "entities_dict['series'][str(series_id)]['series_entities'] = series_entities\n",
    "\n",
    "with open('entities.json', 'w') as json_file:\n",
    "    json.dump(entities_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story_sage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
