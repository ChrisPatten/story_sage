{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "api_key = config['OPENAI_API_KEY']\n",
    "chroma_path = config['CHROMA_PATH']\n",
    "chroma_collection = config['CHROMA_COLLECTION']\n",
    "\n",
    "# Load series.yml to create a mapping from series_metadata_name to series_id\n",
    "with open('series.yml', 'r') as file:\n",
    "    series_list = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run queries through story_sage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from story_sage.story_sage import StorySage\n",
    "\n",
    "# Configure the logger\n",
    "\n",
    "logger = logging.getLogger('story_sage')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it for the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Filter out logs from other modules\n",
    "class StorySageFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name.startswith('story_sage')\n",
    "\n",
    "logger.addFilter(StorySageFilter())\n",
    "\n",
    "# Load all character dictionaries and merge them using the metadata_to_id mapping\n",
    "# Load entities.json\n",
    "with open('entities.json', 'r') as file:\n",
    "    entities = yaml.safe_load(file)\n",
    "\n",
    "story_sage = StorySage(\n",
    "    api_key=api_key,\n",
    "    chroma_path=chroma_path,\n",
    "    chroma_collection_name=chroma_collection,\n",
    "    entities=entities,\n",
    "    series_yml_path='series.yml',\n",
    "    n_chunks=10\n",
    ")\n",
    "\n",
    "\n",
    "# Add a handler to the StorySage logger\n",
    "story_sage.logger = logger\n",
    "\n",
    "def invoke_story_sage(data: dict):\n",
    "    required_keys = ['question', 'book_number', 'chapter_number', 'series_id']\n",
    "    if not all(key in data for key in required_keys):\n",
    "        return {'error': f'Missing parameter! Request must include {\", \".join(required_keys)}'}, 400\n",
    "\n",
    "    try:\n",
    "        result, context = story_sage.invoke(**data)\n",
    "        return result, context\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return {'error': 'Internal server error.'}, 500\n",
    "    \n",
    "data = {\n",
    "    'question': 'Explain the interactions between Cenn and Rand',\n",
    "    'book_number': 2,\n",
    "    'chapter_number': 1,\n",
    "    'series_id': 3\n",
    "}\n",
    "\n",
    "if False:\n",
    "    response, context = invoke_story_sage(data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and send queries to ChromaDB Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from story_sage.story_sage_embedder import StorySageEmbedder\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "class EmbeddingAdapter(SentenceTransformerEmbeddings):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)  \n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self._embed_documents(input)  \n",
    "\n",
    "embedder = EmbeddingAdapter\n",
    "client = chromadb.PersistentClient(path=chroma_path)\n",
    "vector_store = client.get_collection(name=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = {'$and': [\n",
    "                {'$or': [\n",
    "                    {'book_number': {'$lt': 1}},\n",
    "                    {'$and': [\n",
    "                        {'book_number': 1}, \n",
    "                        {'chapter_number': {'$lt': 25}}\n",
    "                    ]}\n",
    "                ]}, \n",
    "                {'a_3_12': True}\n",
    "               ]}\n",
    "\n",
    "filter_dict = {'$or': [\n",
    "                    {'book_number': {'$lt': 1}},\n",
    "                    {'$and': [\n",
    "                        {'book_number': 1}, \n",
    "                        {'chapter_number': {'$lt': 25}}\n",
    "                    ]}\n",
    "                ]}\n",
    "#client.delete_collection('wot_retriever_test')\n",
    "if False:\n",
    "    vector_store.query(query_texts=['trolloc'],\n",
    "                    n_results=5,\n",
    "                    where=filter_dict,\n",
    "                    include=['metadatas','documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "path_to_chunks = './chunks/wheel_of_time/semantic_chunks'\n",
    "chunks = {}\n",
    "for filepath in glob.glob(f'{path_to_chunks}/*.pkl'):\n",
    "    match = re.match(r'(\\d+)_(\\d+)\\.pkl', os.path.basename(filepath))\n",
    "    if match:\n",
    "        book_number, chapter_number = map(int, match.groups())\n",
    "        with open(filepath, 'rb') as f:\n",
    "            if book_number not in chunks:\n",
    "                chunks[book_number] = {}\n",
    "            chunks[book_number][chapter_number] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI based entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Change this to only extract \"people\" and \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "# Create a custom HTTPX client with SSL verification disabled\n",
    "req_client = httpx.Client(verify=False)\n",
    "\n",
    "client = OpenAI(api_key=api_key, http_client=req_client)\n",
    "\n",
    "full_response = None\n",
    "\n",
    "class StorySageEntities(BaseModel):\n",
    "  people: list[str]\n",
    "  places: list[str]\n",
    "  groups: list[str]\n",
    "  animals: list[str]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                extract entities and a summary from text provided to you.\n",
    "                \n",
    "                For example, if the text is:\n",
    "                    Standing with the other Whitecloaks, Perrin saw the Lugard Road near the Manetherendrelle and the border of Murandy.\n",
    "                    If dogs had been able to make footprints on stone, he would have said the tracks were the prints of a pack of large hounds.\n",
    "             \n",
    "                Extract:\n",
    "                    People: Perrin\n",
    "                    Places: Lugard Road, Manetherendrelle, Murandy\n",
    "                    Groups: Whitecloaks, pack\n",
    "                    Animals: dogs\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=StorySageEntities\n",
    "    )\n",
    "\n",
    "    extracted_entity = completion.choices[0].message.parsed\n",
    "    usage_information = completion.usage\n",
    "\n",
    "    return extracted_entity, usage_information\n",
    "\n",
    "\n",
    "#entities = extract_named_entities(chunk_to_extract)\n",
    "#print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run chunks thru OpenAI for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_chapters = 0\n",
    "result = []\n",
    "\n",
    "def extract_entities_from_chunks(chunks):\n",
    "\n",
    "    num_chapters = len(chunks[1])\n",
    "    result = []\n",
    "    \n",
    "    counter = 0\n",
    "    len_cap = 400000\n",
    "    book_chunks = chunks[1]\n",
    "    for i in range(num_chapters):\n",
    "        chapter_chunks = book_chunks[i]\n",
    "        chapter_text = '\\n'.join(chapter_chunks)\n",
    "        chapter_len = len(chapter_text)\n",
    "        if counter + chapter_len > len_cap:\n",
    "            print(f'Waiting for 30 seconds to avoid exceeding the character limit. Current chapter: {i + 1}. Current length: {counter}')\n",
    "            time.sleep(30)\n",
    "            counter = 0\n",
    "        result.append(extract_named_entities(chapter_text))\n",
    "        counter += chapter_len\n",
    "\n",
    "    print(f'Finished extracting from {len(result)} chapters')\n",
    "    if result:\n",
    "        print(result[-5])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump result into a json so I don't have to run this every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if False:\n",
    "    result = extract_entities_from_chunks(chunks)\n",
    "    with open('01_the_eye_of_the_world.json', 'w') as json_file:\n",
    "        json.dump(result, json_file, default=lambda o: o.__dict__, indent=4)\n",
    "\n",
    "with open('01_the_eye_of_the_world.json', 'r') as json_file:\n",
    "    result = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chapters = len(result[0])\n",
    "\n",
    "entities_dict = {\n",
    "    'series': {\n",
    "        'wheel_of_time': {\n",
    "            'series_metadata_name': 'wheel_of_time', \n",
    "            'series_id': 3, \n",
    "            'series_name': 'The Wheel of Time',\n",
    "            'series_entities': {\n",
    "                'people_by_id': {}, \n",
    "                'people_by_name': {}, \n",
    "                'places_by_id': {}, \n",
    "                'places_by_name': {},\n",
    "                'groups_by_id': {},\n",
    "                'groups_by_name': {},\n",
    "                'animals_by_id': {},\n",
    "                'animals_by_name': {}\n",
    "            },\n",
    "            'books': [\n",
    "                {\n",
    "                    'book_name': 'The Eye of the World', \n",
    "                    'book_number': 1, \n",
    "                    'chapter_count': num_chapters,\n",
    "                    'chapters': [],\n",
    "                    'book_entities': {\n",
    "                        'people_by_id': {}, \n",
    "                        'people_by_name': {}, \n",
    "                        'places_by_id': {},\n",
    "                        'places_by_name': {},\n",
    "                        'groups_by_id': {},\n",
    "                        'groups_by_name': {},\n",
    "                        'animals_by_id': {},\n",
    "                        'animals_by_name': {}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, chapter_entities in enumerate(result):\n",
    "    entities_obj = {'chapter': i, 'people': chapter_entities[0]['people'], 'places': chapter_entities[0]['places'], 'groups': chapter_entities[0]['groups'], 'animals': chapter_entities[0]['animals']}\n",
    "    entities_obj['people'] = list(set([str.lower(person.replace('’', \"'\").replace('‘', \"'\")) for person in entities_obj['people']]))\n",
    "    entities_obj['places'] = list(set([str.lower(place.replace('’', \"'\").replace('‘', \"'\")) for place in entities_obj['places']]))\n",
    "    entities_obj['groups'] = list(set([str.lower(group.replace('’', \"'\").replace('‘', \"'\")) for group in entities_obj['groups']]))\n",
    "    entities_obj['animals'] = list(set([str.lower(animal.replace('’', \"'\").replace('‘', \"'\")) for animal in entities_obj['animals']]))\n",
    "    entities_dict['series']['wheel_of_time']['books'][0]['chapters'].append(entities_obj)\n",
    "\n",
    "with open('entities.json', 'w') as json_file:\n",
    "    json.dump(entities_dict, json_file, indent=4)\n",
    "\n",
    "with open('entities.json', 'r') as json_file:\n",
    "    entities_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to cluster similar characters\n",
    "\n",
    "This is an opportunity to make it MUCH smarter. Think things like using semantic understanding to differentiate characters who appear together as separate individuals or other things like that.\n",
    "\n",
    "Maybe do multiple steps? OpenAI cluster, then bounce those against the text to see if it sounds like they're different characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_people = set()\n",
    "all_places = set()\n",
    "all_groups = set()\n",
    "all_animals = set()\n",
    "\n",
    "for book in entities_dict['series']['wheel_of_time']['books']:\n",
    "    for chapter in book['chapters']:\n",
    "        all_people.update(chapter['people'])\n",
    "        all_places.update(chapter['places'])\n",
    "        all_groups.update(chapter['groups'])\n",
    "        all_animals.update(chapter['animals'])\n",
    "\n",
    "class GroupedEntities(BaseModel):\n",
    "    all_people: list[list[str]]\n",
    "\n",
    "def group_similar_names(names_to_group):\n",
    "    text = ', '.join(names_to_group)\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a highly advanced natural language processing agent that \n",
    "                is optimized to do named entity recognition (NER). Your goal is to\n",
    "                group together names that represent the same person from the text provided to you.\n",
    "             \n",
    "                Names usually follow a standard pattern. Haral Luhhan and Alsbet Luhhan are likely to be different people, but Haral Luhhan and Master Luhhan are likely to be the same person.\n",
    "\n",
    "                Make sure all names in the input are present in the output.   \n",
    "             \n",
    "                For example:\n",
    "                    Input: Bran, Mat, Bran al'Vere, Haral Luhhan, Breyan, Matrim Cauthon, Alsbet Luhhan, Master al'Vere, Mat Cauthon\n",
    "                    Output: [['Bran', \"Bran al'Vere\", \"Master al'Vere\"], ['Mat', 'Matrim Cauthon', 'Mat Cauthon'], ['Breyan'], ['Haral Luhhan'], ['Alsbet Luhhan']]\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_format=GroupedEntities\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_entities = group_similar_names(all_people)\n",
    "all_people = grouped_entities.all_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Extracted Entities into Series-level Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create names->id and id->name maps\n",
    "series_entities = entities_dict['series']['wheel_of_time']['series_entities']\n",
    "series_id = 3\n",
    "\n",
    "series_entities['people_by_name'] = {}\n",
    "series_entities['people_by_id'] = {}\n",
    "for idx, group in enumerate(all_people):\n",
    "    id = f\"p_{series_id}_{idx}\"\n",
    "    series_entities['people_by_id'][id] = group\n",
    "    for name in group:\n",
    "        series_entities['people_by_name'][name] = id\n",
    "\n",
    "series_entities['places_by_name'] = {}\n",
    "series_entities['places_by_id'] = {}\n",
    "for idx, place in enumerate(all_places):\n",
    "    id = f\"pl_{series_id}_{idx}\"\n",
    "    series_entities['places_by_id'][id] = place\n",
    "    series_entities['places_by_name'][place] = id\n",
    "\n",
    "series_entities['groups_by_id'] = {}\n",
    "series_entities['groups_by_name'] = {}\n",
    "for idx, group in enumerate(all_groups):\n",
    "    id = f\"g_{series_id}_{idx}\"\n",
    "    series_entities['groups_by_id'][id] = group\n",
    "    series_entities['groups_by_name'][group] = id\n",
    "\n",
    "series_entities['animals_by_id'] = {}\n",
    "series_entities['animals_by_name'] = {}\n",
    "for idx, animal in enumerate(all_animals):\n",
    "    id = f\"a_{series_id}_{idx}\"\n",
    "    series_entities['animals_by_id'][id] = animal\n",
    "    series_entities['animals_by_name'][animal] = id\n",
    "\n",
    "with open('entities.json', 'w') as json_file:\n",
    "    json.dump(entities_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story_sage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
